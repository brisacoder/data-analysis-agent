{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daa7e5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path from JSON: data/InfiAgent-DABench/da-dev-tables/abalone.csv\n",
      "Path object: data\\InfiAgent-DABench\\da-dev-tables\\abalone.csv\n",
      "Path exists: True\n",
      "With native separators: data\\\\InfiAgent-DABench\\\\da-dev-tables\\\\abalone.csv\n",
      "\\nKey points:\n",
      "✓ Windows accepts forward slashes\n",
      "✓ Path() works with forward slashes\n",
      "✓ No conversion needed when reading from JSON\n"
     ]
    }
   ],
   "source": [
    "# Windows handles forward slashes correctly in file paths\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Simulate what happens when you read from JSON\n",
    "json_data = '{\"file_name\": \"data/InfiAgent-DABench/da-dev-tables/abalone.csv\"}'\n",
    "loaded = json.loads(json_data)\n",
    "file_path_from_json = loaded[\"file_name\"]\n",
    "\n",
    "print(f\"Path from JSON: {file_path_from_json}\")\n",
    "\n",
    "# Convert to Path object - works fine with forward slashes\n",
    "path_obj = Path(file_path_from_json)\n",
    "print(f\"Path object: {path_obj}\")\n",
    "print(f\"Path exists: {path_obj.exists()}\")\n",
    "\n",
    "# If you need native Windows separators, you can get them\n",
    "print(f\"With native separators: {path_obj.as_posix().replace('/', '\\\\\\\\')}\")\n",
    "\n",
    "print(\"\\\\nKey points:\")\n",
    "print(\"✓ Windows accepts forward slashes\")\n",
    "print(\"✓ Path() works with forward slashes\") \n",
    "print(\"✓ No conversion needed when reading from JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd6bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "questions = Path(os.getenv(\"QUESTIONS_FILE\"))\n",
    "answers = Path(os.getenv(\"ANSWERS_FILE\"))\n",
    "# df_json = pd.read_json(questions, lines=True).to_dict(orient='records')\n",
    "df_questions = pd.read_json(questions, lines=True)\n",
    "df_answers = pd.read_json(answers, lines=True)\n",
    "df_merged = df_answers.merge(df_questions, left_on=\"id\", right_on=\"id\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c1e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from dataframe_to_dict import parse_dataframe_info\n",
    "\n",
    "def df_info_to_json(df):\n",
    "    buffer = io.StringIO()\n",
    "    df.info(buf=buffer, show_counts=True)\n",
    "    df_json = parse_dataframe_info(buffer.getvalue())\n",
    "    return df_json    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26720fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from planner import create_plan\n",
    "\n",
    "path_prefix = Path(\"data/InfiAgent-DABench/da-dev-tables/\")\n",
    "for index, row in df_merged.iterrows():\n",
    "    file_name = path_prefix / row['file_name']\n",
    "    df = pd.read_csv(file_name)\n",
    "    df_json = df_info_to_json(df)\n",
    "    plan = create_plan(row['question'], df_json, file_name)\n",
    "    df_merged.at[index, 'plan'] = plan.model_dump_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9befeb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"task_list\":[{\"task_name\":\"Setup Imports and Dependencies\",\"details\":\"Import all required libraries such as pandas, numpy, sklearn modules, matplotlib, logging, warnings, datetime, and typing.\",\"dependencies\":\"None\",\"output\":\"All necessary imports ready\",\"assumptions\":\"\"},{\"task_name\":\"Define Configuration Constants\",\"details\":\"Define all constants including RANDOM_SEED=42, file paths (set to \\'data/InfiAgent-DABench/da-dev-tables/my_test_01.csv\\'), thresholds (MISSING_THRESHOLD_ROW=0.5, MISSING_THRESHOLD_COL=0.8), and any analysis-specific parameters.\",\"dependencies\":\"Task 1\",\"output\":\"Configuration constants defined\",\"assumptions\":\"Using the file path specified in the user input.\"},{\"task_name\":\"Setup Logging and Reproducibility\",\"details\":\"Configure logging with timestamp format, set random seeds (np.random.seed(42)), suppress warnings if needed.\",\"dependencies\":\"Tasks 1-2\",\"output\":\"Logging configured, reproducibility ensured\",\"assumptions\":\"\"},{\"task_name\":\"Load and Validate Input Data\",\"details\":\"Load data from \\'data/InfiAgent-DABench/da-dev-tables/my_test_01.csv\\', log initial shape, check for basic validity such as presence of all expected columns and correct data types.\",\"dependencies\":\"Tasks 1-3\",\"output\":\"Raw DataFrame loaded and initially validated\",\"assumptions\":\"Assume that the file exists at the specified location and columns match the structure provided.\"},{\"task_name\":\"Data Quality Assessment and Cleaning\",\"details\":\"Check missing values, duplicates, and data types for all columns. Log data quality metrics. Handle missing data according to thresholds defined in Task 2. Apply cleaning as necessary.\",\"dependencies\":\"Tasks 1-4\",\"output\":\"Cleaned DataFrame with quality report logged\",\"assumptions\":\"Assume low missingness due to non-null counts matching row count, but still check for potential missing values.\"},{\"task_name\":\"Replace Missing Values in MedInc with Mean\",\"details\":\"Identify any missing values in the \\'MedInc\\' column and replace them with the mean value of that column. Log the number of replacements.\",\"dependencies\":\"Task 5\",\"output\":\"\\'MedInc\\' column with missing values replaced by mean\",\"assumptions\":\"Although the data structure indicates no missing values, this step is performed as specified.\"},{\"task_name\":\"Standardize AveOccup Column Using Z-Scores\",\"details\":\"Standardize the values in the \\'AveOccup\\' column by subtracting the mean and dividing by the standard deviation (z-score standardization). Store the standardized values in a new column called \\'AveOccup_zscore\\'.\",\"dependencies\":\"Task 5\",\"output\":\"DataFrame with \\'AveOccup_zscore\\' column added\",\"assumptions\":\"Standardization is done with respect to the whole column (population statistics, not sample).\"},{\"task_name\":\"Create RoomsPerPerson Feature\",\"details\":\"Create a new feature called \\'RoomsPerPerson\\' by dividing the values in the \\'AveRooms\\' column by the values in the \\'Population\\' column. Handle division by zero by assigning NaN or another appropriate value. Log the number of any NaN or infinite values created.\",\"dependencies\":\"Task 5\",\"output\":\"DataFrame with \\'RoomsPerPerson\\' column added\",\"assumptions\":\"If \\'Population\\' is zero, set \\'RoomsPerPerson\\' to NaN.\"},{\"task_name\":\"Calculate Pearson Correlation Between MedianHouseValue and RoomsPerPerson\",\"details\":\"Calculate the Pearson correlation coefficient between \\'MedianHouseValue\\' and the newly created \\'RoomsPerPerson\\' column. Log the coefficient value and its interpretation.\",\"dependencies\":\"Tasks 8\",\"output\":\"Pearson correlation coefficient between \\'MedianHouseValue\\' and \\'RoomsPerPerson\\'\",\"assumptions\":\"Assume sufficient non-missing data for calculation.\"},{\"task_name\":\"Calculate Mean and Standard Deviation of MedianHouseValue\",\"details\":\"Compute the mean and standard deviation for the \\'MedianHouseValue\\' column. Log and report these statistics clearly.\",\"dependencies\":\"Task 5\",\"output\":\"Mean and standard deviation of \\'MedianHouseValue\\'\",\"assumptions\":\"Assume no missing values in the target column.\"}]}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.loc[14, 'plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f00629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"data/merged_with_plans.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44221fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coder import create_code\n",
    "\n",
    "path_prefix = Path(\"data/InfiAgent-DABench/da-dev-tables/\")\n",
    "for index, row in df_merged.iterrows():\n",
    "    file_name = path_prefix / row['file_name']\n",
    "    df = pd.read_csv(file_name)\n",
    "    df_json = df_info_to_json(df)\n",
    "    code = create_code(row['plan'], row['question'], df_json, file_name)\n",
    "    df_merged.at[index, 'code'] = code.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43ad021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"data/merged_with_code.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccb090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
