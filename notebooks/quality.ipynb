{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f535baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "df = pd.read_csv('../curated/DEVRT-DACIA-SPRING.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08c19469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_analysis_agent.automotive_data_quality import generate_automotive_quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60cb37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_analysis_agent.automotive_data_quality:JSON report saved to report.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running automotive quality report with JSON output...\n",
      "Report completed! Check report.json for results.\n"
     ]
    }
   ],
   "source": [
    "# Final demonstration: Silent mode when writing to files\n",
    "print(\"Running automotive quality report with JSON output...\")\n",
    "generate_automotive_quality_report(df, json_output_file='report.json')\n",
    "print(\"Report completed! Check report.json for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad534d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_json('../curated/15_DEVRT-DACIA-SPRING-questions.jsonl', lines=True)\n",
    "df_answers = pd.read_json('../curated/15_DEVRT-DACIA-SPRING-labels.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31f5c27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    What is the correlation coefficient between al...\n",
       " 1    What is the average altitude I drive at? Fill ...\n",
       " 2    Which drivers achieve the best energy efficien...\n",
       " 3    What is the average state of health (SOH) perc...\n",
       " 4    What is the correlation coefficient between fr...\n",
       " Name: question, dtype: object,\n",
       " 0        {'altitude_speed_correlation': -0.03}\n",
       " 1           {'average_altitude_meters': 135.1}\n",
       " 2    {'efficient_drivers': ['d1', 'd2', 'd3']}\n",
       " 3                  {'fleet_average_soh': 98.5}\n",
       " 4    {'frontal_wind_speed_correlation': 0.057}\n",
       " Name: common_answers, dtype: object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['question'].head(), df_answers['common_answers'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51b4e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# 1) Upload the JSON and create a vector store\n",
    "client = OpenAI()  # requires OPENAI_API_KEY\n",
    "\n",
    "upload_report = client.files.create(\n",
    "    file=open(\"report.json\", \"rb\"),\n",
    "    purpose=\"user_data\",\n",
    ")\n",
    "\n",
    "upload_csv = client.files.create(\n",
    "    file=open(\"../curated/DEVRT-DACIA-SPRING.csv\", \"rb\"),\n",
    "    purpose=\"user_data\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13fcfd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FileObject(id='file-MUUbFY99D7otoy1VxvfcUR', bytes=69264, created_at=1754725273, filename='report.json', object='file', purpose='user_data', status='processed', expires_at=None, status_details=None),\n",
       " FileObject(id='file-UEosua5u86rkKhVexb1uLX', bytes=3006479, created_at=1754725274, filename='DEVRT-DACIA-SPRING.csv', object='file', purpose='user_data', status='processed', expires_at=None, status_details=None))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_report, upload_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4633d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "\"You are an automotive data analysis expert with access to a code execution environment. \"\n",
    "\"You will receive two files: \"\n",
    "\"1) A CSV file containing automotive telemetry data with sensor readings, timestamps, and vehicle parameters \"\n",
    "\"2) A JSON report containing comprehensive data quality assessment including missing values, outliers, signal validation, and statistical summaries. \"\n",
    "\"Use the code interpreter to read, analyze, and visualize this data. \"\n",
    "\"When answering questions: \"\n",
    "\"- Always examine both files programmatically to understand the data structure \"\n",
    "\"- Reference specific findings from the quality report when relevant \"\n",
    "\"- Create visualizations and statistical analyses as needed \"\n",
    "\"- Provide automotive domain-specific insights about signal patterns, anomalies, and data integrity \"\n",
    "\"- If you need to perform calculations or data transformations, write and execute code \"\n",
    "\"- Be precise and cite specific data points from your analysis \"\n",
    "\"Answer accurately based on the data; if something is unknown or unclear, say so explicitly.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8ae31c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309.75\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    tools=[{\n",
    "      \"type\": \"code_interpreter\",\n",
    "      \"container\": {\"type\": \"auto\",\n",
    "                    \"file_ids\": [upload_csv.id, upload_report.id]}\n",
    "    }],\n",
    "    input=[\n",
    "        {   \n",
    "            \"role\": \"developer\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": df_questions['question'][7],\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "267f8f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json, pandas as pd, numpy as np, math, os, textwrap, sys, statistics, re, warningscsv_path = \"/mnt/data/file-UEosua5u86rkKhVexb1uLX-DEVRT-DACIA-SPRING.csv\"\n",
      "json_path = \"/mnt/data/file-MUUbFY99D7otoy1VxvfcUR-report.json\"\n",
      "\n",
      "# Load files\n",
      "try:\n",
      "    df = pd.read_csv(csv_path)\n",
      "except Exception as e:\n",
      "    df = None\n",
      "    csv_err = str(e)\n",
      "\n",
      "try:\n",
      "    with open(json_path, \"r\") as f:\n",
      "        report = json.load(f)\n",
      "except Exception as e:\n",
      "    report = None\n",
      "    json_err = str(e)\n",
      "\n",
      "(type(df), len(df) if df is not None else None, type(report) if report is not None else None)# Inspect columns\n",
      "cols = list(df.columns) if df is not None else []\n",
      "colsimport pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df_work = df.copy()\n",
      "\n",
      "# Ensure numeric types\n",
      "for col in [\"speedAvg\", \"soh\"]:\n",
      "    if col in df_work.columns:\n",
      "        df_work[col] = pd.to_numeric(df_work[col], errors=\"coerce\")\n",
      "\n",
      "# Fill missing speedAvg with mean\n",
      "if \"speedAvg\" in df_work.columns:\n",
      "    mean_speed = df_work[\"speedAvg\"].mean(skipna=True)\n",
      "    df_work[\"speedAvg\"] = df_work[\"speedAvg\"].fillna(mean_speed)\n",
      "\n",
      "# Define bins and labels\n",
      "bins = [0, 50, 80, 100, np.inf]\n",
      "labels = [\"0-50 km/h\", \"50-80 km/h\", \"80-100 km/h\", \"100+ km/h\"]\n",
      "\n",
      "# Assign bins\n",
      "# We consider only non-negative speeds; negative speeds will be clipped to 0 for binning\n",
      "if \"speedAvg\" in df_work.columns:\n",
      "    speeds = df_work[\"speedAvg\"].clip(lower=0)\n",
      "    df_work[\"speed_bin\"] = pd.cut(speeds, bins=bins, labels=labels, right=False, include_lowest=True)\n",
      "\n",
      "# Compute average SOH per bin\n",
      "if \"soh\" in df_work.columns and \"speed_bin\" in df_work.columns:\n",
      "    soh_by_bin = df_work.groupby(\"speed_bin\")[\"soh\"].mean()\n",
      "    # Round to 2 decimals\n",
      "    soh_by_bin_rounded = soh_by_bin.round(2)\n",
      "    soh_by_bin, soh_by_bin_rounded, mean_speed, df_work[\"speed_bin\"].isna().sum()\n",
      "else:\n",
      "    soh_by_bin = None# Let's compute and return the series to view\n",
      "soh_by_bin_dict = None\n",
      "soh_by_bin_rounded_dict = None\n",
      "mean_speed_val = None\n",
      "n_na_speed_bin = None\n",
      "\n",
      "if \"soh\" in df_work.columns and \"speed_bin\" in df_work.columns:\n",
      "    soh_by_bin = df_work.groupby(\"speed_bin\")[\"soh\"].mean()\n",
      "    soh_by_bin_rounded = soh_by_bin.round(2)\n",
      "    soh_by_bin_dict = soh_by_bin.to_dict()\n",
      "    soh_by_bin_rounded_dict = soh_by_bin_rounded.to_dict()\n",
      "    mean_speed_val = float(df_work[\"speedAvg\"].mean())\n",
      "    n_na_speed_bin = int(df_work[\"speed_bin\"].isna().sum())\n",
      "\n",
      "soh_by_bin_dict, soh_by_bin_rounded_dict, mean_speed_val, n_na_speed_bin# Select bin with highest SOH based on rounded means\n",
      "if soh_by_bin_dict is not None:\n",
      "    soh_rounded = pd.Series(soh_by_bin_rounded_dict)\n",
      "    # Ensure ordering by our labels\n",
      "    soh_rounded = soh_rounded.reindex(labels)\n",
      "    optimal_bin = soh_rounded.idxmax() if not soh_rounded.dropna().empty else None\n",
      "else:\n",
      "    optimal_bin = None\n",
      "\n",
      "optimal_bin\n"
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ResponseCodeInterpreterToolCall\n",
    "print(''.join([item.code for item in response.output if isinstance(item, ResponseCodeInterpreterToolCall)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, numpy as np, math, os, textwrap, sys, statistics, re, warningscsv_path = \"/mnt/data/file-UEosua5u86rkKhVexb1uLX-DEVRT-DACIA-SPRING.csv\"\n",
    "json_path = \"/mnt/data/file-MUUbFY99D7otoy1VxvfcUR-report.json\"\n",
    "\n",
    "# Load files\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "except Exception as e:\n",
    "    df = None\n",
    "    csv_err = str(e)\n",
    "\n",
    "try:\n",
    "    with open(json_path, \"r\") as f:\n",
    "        report = json.load(f)\n",
    "except Exception as e:\n",
    "    report = None\n",
    "    json_err = str(e)\n",
    "\n",
    "(type(df), len(df) if df is not None else None, type(report) if report is not None else None)# Inspect columns\n",
    "cols = list(df.columns) if df is not None else []\n",
    "colsimport pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_work = df.copy()\n",
    "\n",
    "# Ensure numeric types\n",
    "for col in [\"speedAvg\", \"soh\"]:\n",
    "    if col in df_work.columns:\n",
    "        df_work[col] = pd.to_numeric(df_work[col], errors=\"coerce\")\n",
    "\n",
    "# Fill missing speedAvg with mean\n",
    "if \"speedAvg\" in df_work.columns:\n",
    "    mean_speed = df_work[\"speedAvg\"].mean(skipna=True)\n",
    "    df_work[\"speedAvg\"] = df_work[\"speedAvg\"].fillna(mean_speed)\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0, 50, 80, 100, np.inf]\n",
    "labels = [\"0-50 km/h\", \"50-80 km/h\", \"80-100 km/h\", \"100+ km/h\"]\n",
    "\n",
    "# Assign bins\n",
    "# We consider only non-negative speeds; negative speeds will be clipped to 0 for binning\n",
    "if \"speedAvg\" in df_work.columns:\n",
    "    speeds = df_work[\"speedAvg\"].clip(lower=0)\n",
    "    df_work[\"speed_bin\"] = pd.cut(speeds, bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "# Compute average SOH per bin\n",
    "if \"soh\" in df_work.columns and \"speed_bin\" in df_work.columns:\n",
    "    soh_by_bin = df_work.groupby(\"speed_bin\")[\"soh\"].mean()\n",
    "    # Round to 2 decimals\n",
    "    soh_by_bin_rounded = soh_by_bin.round(2)\n",
    "    soh_by_bin, soh_by_bin_rounded, mean_speed, df_work[\"speed_bin\"].isna().sum()\n",
    "else:\n",
    "    soh_by_bin = None# Let's compute and return the series to view\n",
    "soh_by_bin_dict = None\n",
    "soh_by_bin_rounded_dict = None\n",
    "mean_speed_val = None\n",
    "n_na_speed_bin = None\n",
    "\n",
    "if \"soh\" in df_work.columns and \"speed_bin\" in df_work.columns:\n",
    "    soh_by_bin = df_work.groupby(\"speed_bin\")[\"soh\"].mean()\n",
    "    soh_by_bin_rounded = soh_by_bin.round(2)\n",
    "    soh_by_bin_dict = soh_by_bin.to_dict()\n",
    "    soh_by_bin_rounded_dict = soh_by_bin_rounded.to_dict()\n",
    "    mean_speed_val = float(df_work[\"speedAvg\"].mean())\n",
    "    n_na_speed_bin = int(df_work[\"speed_bin\"].isna().sum())\n",
    "\n",
    "soh_by_bin_dict, soh_by_bin_rounded_dict, mean_speed_val, n_na_speed_bin# Select bin with highest SOH based on rounded means\n",
    "if soh_by_bin_dict is not None:\n",
    "    soh_rounded = pd.Series(soh_by_bin_rounded_dict)\n",
    "    # Ensure ordering by our labels\n",
    "    soh_rounded = soh_rounded.reindex(labels)\n",
    "    optimal_bin = soh_rounded.idxmax() if not soh_rounded.dropna().empty else None\n",
    "else:\n",
    "    optimal_bin = None\n",
    "\n",
    "optimal_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88199c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "deleted_csv = client.files.delete(upload_csv.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_report = client.files.delete(upload_report.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af053cf1",
   "metadata": {},
   "source": [
    "# Automotive Data Quality API Demo\n",
    "\n",
    "This notebook demonstrates how to use the new Automotive Data Quality API to query specific column information from quality reports.\n",
    "\n",
    "The API allows you to:\n",
    "- Load JSON reports with or without the original CSV data\n",
    "- Get comprehensive information about specific columns\n",
    "- Query correlations, signal types, and quality issues\n",
    "- Search and filter columns by various criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74aecf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API initialized successfully!\n",
      "Overall quality score: 0.846\n",
      "Total columns analyzed: 39\n"
     ]
    }
   ],
   "source": [
    "# Import the new API\n",
    "from data_analysis_agent.automotive_data_quality_api import AutomotiveDataQualityAPI, load_quality_report\n",
    "\n",
    "# Initialize the API with the existing report and CSV data\n",
    "api = AutomotiveDataQualityAPI(\n",
    "    report_path='report.json',\n",
    "    csv_path='../curated/DEVRT-DACIA-SPRING.csv'\n",
    ")\n",
    "\n",
    "print(f\"API initialized successfully!\")\n",
    "print(f\"Overall quality score: {api.get_overall_quality_score():.3f}\")\n",
    "print(f\"Total columns analyzed: {len(api.list_all_columns())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45ec63ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comprehensive Information for 'speedAvg' ===\n",
      "Data type: float64\n",
      "Missing percentage: 1.29%\n",
      "Unique values: 78\n",
      "\n",
      "Signal Dictionary:\n",
      "  Expanded name: speedAvg\n",
      "  Domain: Unknown\n",
      "\n",
      "Range Validation:\n",
      "  Detected signal type: SPEED\n",
      "  Has violations: False\n",
      "\n",
      "Quality Assessment:\n",
      "  Overall score: 1.000/1.0\n",
      "  Issues: []\n",
      "  Strengths: ['Low missing value rate: 1.29%']\n"
     ]
    }
   ],
   "source": [
    "# Get comprehensive information about a specific column\n",
    "column_name = 'speedAvg'\n",
    "column_info = api.get_column_info(column_name)\n",
    "\n",
    "print(f\"=== Comprehensive Information for '{column_name}' ===\")\n",
    "print(f\"Data type: {column_info['data_type']}\")\n",
    "print(f\"Missing percentage: {column_info['missing_percentage']}%\")\n",
    "print(f\"Unique values: {column_info['unique_count']}\")\n",
    "\n",
    "# Signal dictionary information\n",
    "signal_dict = column_info['signal_dictionary']\n",
    "print(f\"\\nSignal Dictionary:\")\n",
    "print(f\"  Expanded name: {signal_dict.get('expanded', 'N/A')}\")\n",
    "print(f\"  Domain: {signal_dict.get('domain', 'N/A')}\")\n",
    "\n",
    "# Range validation\n",
    "range_val = column_info.get('range_validation', {})\n",
    "if range_val.get('signal_type'):\n",
    "    print(f\"\\nRange Validation:\")\n",
    "    print(f\"  Detected signal type: {range_val['signal_type']}\")\n",
    "    print(f\"  Has violations: {range_val.get('has_violations', False)}\")\n",
    "\n",
    "# Quality assessment\n",
    "quality = column_info.get('quality_assessment', {})\n",
    "print(f\"\\nQuality Assessment:\")\n",
    "print(f\"  Overall score: {quality.get('overall_score', 0):.3f}/1.0\")\n",
    "print(f\"  Issues: {quality.get('issues', [])}\")\n",
    "print(f\"  Strengths: {quality.get('strengths', [])}\")\n",
    "\n",
    "# Priority issues related to this column\n",
    "priority_issues = column_info.get('priority_issues', [])\n",
    "if priority_issues:\n",
    "    print(f\"\\nPriority Issues:\")\n",
    "    for issue in priority_issues:\n",
    "        print(f\"  ⚠️ {issue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1248f1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Human-Readable Column Summary ===\n",
      "Column: speedAvg\n",
      "Domain: Unknown\n",
      "Data type: float64\n",
      "Missing values: 1.29%\n",
      "Unique values: 78\n",
      "Detected as: SPEED\n",
      "Quality score: 1.00/1.0\n"
     ]
    }
   ],
   "source": [
    "# Get a human-readable summary for the column\n",
    "print(\"=== Human-Readable Column Summary ===\")\n",
    "summary = api.get_column_summary('speedAvg')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b67c9401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Problematic Columns Analysis ===\n",
      "\n",
      "High Missing: 3 columns\n",
      "  • cars_by_speed_interval_0_80\n",
      "  • cars_by_speed_interval_80_100\n",
      "  • cars_by_speed_interval_100_120\n",
      "\n",
      "Range Violations: 6 columns\n",
      "  • cars_by_speed_interval_80_100\n",
      "  • cars_by_speed_interval_100_120\n",
      "  • cars_by_speed_interval_0_50\n",
      "  • cars_by_speed_interval_50_80\n",
      "  • cars_by_speed_interval_80_120\n",
      "  ... and 1 more\n",
      "\n",
      "Constant Values: 3 columns\n",
      "  • soh\n",
      "  • capacity\n",
      "  • ref_consumption\n",
      "\n",
      "Saturated: 5 columns\n",
      "  • car_id\n",
      "  • wind_mph\n",
      "  • wind_kph\n",
      "  • cars_by_speed_interval_0_80\n",
      "  • cars_by_speed_interval_120_inf\n",
      "\n",
      "=== Overall Priority Issues ===\n",
      "⚠️ Column 'cars_by_speed_interval_0_80': 70.2% missing values\n",
      "⚠️ Column 'cars_by_speed_interval_80_100': 67.8% missing values\n",
      "⚠️ Column 'cars_by_speed_interval_80_100': 4.6% values outside physically possible range\n",
      "⚠️ Column 'cars_by_speed_interval_100_120': 65.6% missing values\n",
      "⚠️ Column 'cars_by_speed_interval_100_120': 16.4% values outside physically possible range\n",
      "... and 3 more issues\n"
     ]
    }
   ],
   "source": [
    "# Find problematic columns based on different criteria\n",
    "print(\"=== Problematic Columns Analysis ===\")\n",
    "\n",
    "problems = api.get_problematic_columns(\n",
    "    min_missing_pct=50.0,  # Columns with >50% missing values\n",
    "    min_violation_pct=1.0   # Columns with >1% range violations\n",
    ")\n",
    "\n",
    "for issue_type, columns in problems.items():\n",
    "    if columns:\n",
    "        print(f\"\\n{issue_type.replace('_', ' ').title()}: {len(columns)} columns\")\n",
    "        for col in columns[:5]:  # Show first 5\n",
    "            print(f\"  • {col}\")\n",
    "        if len(columns) > 5:\n",
    "            print(f\"  ... and {len(columns) - 5} more\")\n",
    "\n",
    "# Get overall priority issues\n",
    "print(f\"\\n=== Overall Priority Issues ===\")\n",
    "priority_issues = api.get_priority_issues()\n",
    "for issue in priority_issues[:5]:  # Show first 5\n",
    "    print(f\"⚠️ {issue}\")\n",
    "\n",
    "if len(priority_issues) > 5:\n",
    "    print(f\"... and {len(priority_issues) - 5} more issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dfd7f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Correlation Analysis ===\n",
      "Unexpected correlations found: 1\n",
      "  wind_mph ↔ wind_kph: 1.000\n",
      "\n",
      "Correlations involving 'speedAvg': 0\n",
      "\n",
      "=== Column Search ===\n",
      "Columns containing 'speed': ['speedAvg', 'cars_by_speed_interval_0_80', 'cars_by_speed_interval_80_100', 'cars_by_speed_interval_100_120', 'cars_by_speed_interval_0_50', 'cars_by_speed_interval_50_80', 'cars_by_speed_interval_80_120', 'cars_by_speed_interval_120_inf', 'max_speed']\n",
      "Columns containing 'wind': ['wind_mph', 'wind_kph', 'wind_degree', 'wind_dir', 'Frontal_Wind']\n",
      "\n",
      "Latitude columns: ['latitude']\n",
      "Longitude columns: ['longitude']\n"
     ]
    }
   ],
   "source": [
    "# Analyze correlations\n",
    "print(\"=== Correlation Analysis ===\")\n",
    "\n",
    "# Get all unexpected correlations\n",
    "unexpected_corrs = api.get_correlations(correlation_type='unexpected')\n",
    "print(f\"Unexpected correlations found: {len(unexpected_corrs)}\")\n",
    "\n",
    "for corr in unexpected_corrs[:3]:  # Show first 3\n",
    "    print(f\"  {corr['column1']} ↔ {corr['column2']}: {corr['correlation']:.3f}\")\n",
    "\n",
    "# Get correlations for a specific column\n",
    "speed_corrs = api.get_correlations(column_name='speedAvg')\n",
    "print(f\"\\nCorrelations involving 'speedAvg': {len(speed_corrs)}\")\n",
    "\n",
    "# Search for columns\n",
    "print(f\"\\n=== Column Search ===\")\n",
    "speed_columns = api.search_columns('speed')\n",
    "print(f\"Columns containing 'speed': {speed_columns}\")\n",
    "\n",
    "wind_columns = api.search_columns('wind')\n",
    "print(f\"Columns containing 'wind': {wind_columns}\")\n",
    "\n",
    "# Get columns by signal type (if any are detected)\n",
    "latitude_cols = api.get_columns_by_signal_type('LATITUDE')\n",
    "longitude_cols = api.get_columns_by_signal_type('LONGITUDE')\n",
    "print(f\"\\nLatitude columns: {latitude_cols}\")\n",
    "print(f\"Longitude columns: {longitude_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b9df1e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Convenience Functions ===\n",
      "Quick summary for 'soc' column:\n",
      "Column: soc\n",
      "Domain: Unknown\n",
      "Data type: int64\n",
      "Missing values: 0.0%\n",
      "Unique values: 60\n",
      "Quality score: 1.00/1.0\n",
      "\n",
      "=== Dataset Statistics ===\n",
      "Shape: [8423, 39]\n",
      "Memory usage: 7.58 MB\n",
      "Missing percentage: 8.19%\n",
      "Numeric columns: 28\n",
      "Non-numeric columns: 11\n",
      "\n",
      "=== All Available Columns ===\n",
      "Total columns: 39\n",
      "First 10 columns: ['soc', 'soh', 'route_id', 'longitude', 'latitude', 'altitude', 'timestamp_gps_utc', 'car_id', 'time_diff', 'point_geom']\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate convenience functions\n",
    "from data_analysis_agent.automotive_data_quality_api import get_column_quality_summary\n",
    "\n",
    "print(\"=== Convenience Functions ===\")\n",
    "\n",
    "# Quick column summary without creating API instance\n",
    "quick_summary = get_column_quality_summary('report.json', 'soc')\n",
    "print(\"Quick summary for 'soc' column:\")\n",
    "print(quick_summary)\n",
    "\n",
    "# Show basic dataset statistics\n",
    "print(f\"\\n=== Dataset Statistics ===\")\n",
    "basic_stats = api.get_basic_stats()\n",
    "print(f\"Shape: {basic_stats['shape']}\")\n",
    "print(f\"Memory usage: {basic_stats['memory_usage_mb']} MB\")\n",
    "print(f\"Missing percentage: {basic_stats['missing_percentage']}%\")\n",
    "print(f\"Numeric columns: {basic_stats['numeric_columns']}\")\n",
    "print(f\"Non-numeric columns: {basic_stats['non_numeric_columns']}\")\n",
    "\n",
    "# List all available columns\n",
    "print(f\"\\n=== All Available Columns ===\")\n",
    "all_columns = api.list_all_columns()\n",
    "print(f\"Total columns: {len(all_columns)}\")\n",
    "print(\"First 10 columns:\", all_columns[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
