{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f535baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "df = pd.read_csv('../curated/DEVRT-DACIA-SPRING.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08c19469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_analysis_agent.automotive_data_quality import generate_automotive_quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60cb37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_analysis_agent.automotive_data_quality:JSON report saved to report.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running automotive quality report with JSON output...\n",
      "Report completed! Check report.json for results.\n"
     ]
    }
   ],
   "source": [
    "# Final demonstration: Silent mode when writing to files\n",
    "print(\"Running automotive quality report with JSON output...\")\n",
    "generate_automotive_quality_report(df, json_output_file='report.json')\n",
    "print(\"Report completed! Check report.json for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad534d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_json('../curated/15_DEVRT-DACIA-SPRING-questions.jsonl', lines=True)\n",
    "df_answers = pd.read_json('../curated/15_DEVRT-DACIA-SPRING-labels.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31f5c27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    What is the correlation coefficient between al...\n",
       " 1    What is the average altitude I drive at? Fill ...\n",
       " 2    Which drivers achieve the best energy efficien...\n",
       " 3    What is the average state of health (SOH) perc...\n",
       " 4    What is the correlation coefficient between fr...\n",
       " Name: question, dtype: object,\n",
       " 0        {'altitude_speed_correlation': -0.03}\n",
       " 1           {'average_altitude_meters': 135.1}\n",
       " 2    {'efficient_drivers': ['d1', 'd2', 'd3']}\n",
       " 3                  {'fleet_average_soh': 98.5}\n",
       " 4    {'frontal_wind_speed_correlation': 0.057}\n",
       " Name: common_answers, dtype: object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['question'].head(), df_answers['common_answers'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51b4e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# 1) Upload the JSON and create a vector store\n",
    "client = OpenAI()  # requires OPENAI_API_KEY\n",
    "\n",
    "upload_report = client.files.create(\n",
    "    file=open(\"report.json\", \"rb\"),\n",
    "    purpose=\"user_data\",\n",
    ")\n",
    "\n",
    "upload_csv = client.files.create(\n",
    "    file=open(\"../curated/DEVRT-DACIA-SPRING.csv\", \"rb\"),\n",
    "    purpose=\"user_data\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13fcfd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FileObject(id='file-MUUbFY99D7otoy1VxvfcUR', bytes=69264, created_at=1754725273, filename='report.json', object='file', purpose='user_data', status='processed', expires_at=None, status_details=None),\n",
       " FileObject(id='file-UEosua5u86rkKhVexb1uLX', bytes=3006479, created_at=1754725274, filename='DEVRT-DACIA-SPRING.csv', object='file', purpose='user_data', status='processed', expires_at=None, status_details=None))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_report, upload_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4633d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "\"You are an automotive data analysis expert with access to a code execution environment. \"\n",
    "\"You will receive two files: \"\n",
    "\"1) A CSV file containing automotive telemetry data with sensor readings, timestamps, and vehicle parameters \"\n",
    "\"2) A JSON report containing comprehensive data quality assessment including missing values, outliers, signal validation, and statistical summaries. \"\n",
    "\"Use the code interpreter to read, analyze, and visualize this data. \"\n",
    "\"When answering questions: \"\n",
    "\"- Always examine both files programmatically to understand the data structure \"\n",
    "\"- Reference specific findings from the quality report when relevant \"\n",
    "\"- Create visualizations and statistical analyses as needed \"\n",
    "\"- Provide automotive domain-specific insights about signal patterns, anomalies, and data integrity \"\n",
    "\"- If you need to perform calculations or data transformations, write and execute code \"\n",
    "\"- Be precise and cite specific data points from your analysis \"\n",
    "\"Answer accurately based on the data; if something is unknown or unclear, say so explicitly.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8ae31c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309.75\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    tools=[{\n",
    "      \"type\": \"code_interpreter\",\n",
    "      \"container\": {\"type\": \"auto\",\n",
    "                    \"file_ids\": [upload_csv.id, upload_report.id]}\n",
    "    }],\n",
    "    input=[\n",
    "        {   \n",
    "            \"role\": \"developer\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": df_questions['question'][7],\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "267f8f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json, pandas as pd, numpy as np, math, os, textwrap, sys, statistics, re, warningscsv_path = \"/mnt/data/file-UEosua5u86rkKhVexb1uLX-DEVRT-DACIA-SPRING.csv\"\n",
      "json_path = \"/mnt/data/file-MUUbFY99D7otoy1VxvfcUR-report.json\"\n",
      "\n",
      "# Load files\n",
      "try:\n",
      "    df = pd.read_csv(csv_path)\n",
      "except Exception as e:\n",
      "    df = None\n",
      "    csv_err = str(e)\n",
      "\n",
      "try:\n",
      "    with open(json_path, \"r\") as f:\n",
      "        report = json.load(f)\n",
      "except Exception as e:\n",
      "    report = None\n",
      "    json_err = str(e)\n",
      "\n",
      "(type(df), len(df) if df is not None else None, type(report) if report is not None else None)# Inspect columns\n",
      "cols = list(df.columns) if df is not None else []\n",
      "colsimport pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df_work = df.copy()\n",
      "\n",
      "# Ensure numeric types\n",
      "for col in [\"speedAvg\", \"soh\"]:\n",
      "    if col in df_work.columns:\n",
      "        df_work[col] = pd.to_numeric(df_work[col], errors=\"coerce\")\n",
      "\n",
      "# Fill missing speedAvg with mean\n",
      "if \"speedAvg\" in df_work.columns:\n",
      "    mean_speed = df_work[\"speedAvg\"].mean(skipna=True)\n",
      "    df_work[\"speedAvg\"] = df_work[\"speedAvg\"].fillna(mean_speed)\n",
      "\n",
      "# Define bins and labels\n",
      "bins = [0, 50, 80, 100, np.inf]\n",
      "labels = [\"0-50 km/h\", \"50-80 km/h\", \"80-100 km/h\", \"100+ km/h\"]\n",
      "\n",
      "# Assign bins\n",
      "# We consider only non-negative speeds; negative speeds will be clipped to 0 for binning\n",
      "if \"speedAvg\" in df_work.columns:\n",
      "    speeds = df_work[\"speedAvg\"].clip(lower=0)\n",
      "    df_work[\"speed_bin\"] = pd.cut(speeds, bins=bins, labels=labels, right=False, include_lowest=True)\n",
      "\n",
      "# Compute average SOH per bin\n",
      "if \"soh\" in df_work.columns and \"speed_bin\" in df_work.columns:\n",
      "    soh_by_bin = df_work.groupby(\"speed_bin\")[\"soh\"].mean()\n",
      "    # Round to 2 decimals\n",
      "    soh_by_bin_rounded = soh_by_bin.round(2)\n",
      "    soh_by_bin, soh_by_bin_rounded, mean_speed, df_work[\"speed_bin\"].isna().sum()\n",
      "else:\n",
      "    soh_by_bin = None# Let's compute and return the series to view\n",
      "soh_by_bin_dict = None\n",
      "soh_by_bin_rounded_dict = None\n",
      "mean_speed_val = None\n",
      "n_na_speed_bin = None\n",
      "\n",
      "if \"soh\" in df_work.columns and \"speed_bin\" in df_work.columns:\n",
      "    soh_by_bin = df_work.groupby(\"speed_bin\")[\"soh\"].mean()\n",
      "    soh_by_bin_rounded = soh_by_bin.round(2)\n",
      "    soh_by_bin_dict = soh_by_bin.to_dict()\n",
      "    soh_by_bin_rounded_dict = soh_by_bin_rounded.to_dict()\n",
      "    mean_speed_val = float(df_work[\"speedAvg\"].mean())\n",
      "    n_na_speed_bin = int(df_work[\"speed_bin\"].isna().sum())\n",
      "\n",
      "soh_by_bin_dict, soh_by_bin_rounded_dict, mean_speed_val, n_na_speed_bin# Select bin with highest SOH based on rounded means\n",
      "if soh_by_bin_dict is not None:\n",
      "    soh_rounded = pd.Series(soh_by_bin_rounded_dict)\n",
      "    # Ensure ordering by our labels\n",
      "    soh_rounded = soh_rounded.reindex(labels)\n",
      "    optimal_bin = soh_rounded.idxmax() if not soh_rounded.dropna().empty else None\n",
      "else:\n",
      "    optimal_bin = None\n",
      "\n",
      "optimal_bin\n"
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ResponseCodeInterpreterToolCall\n",
    "print(''.join([item.code for item in response.output if isinstance(item, ResponseCodeInterpreterToolCall)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, numpy as np, math, os, textwrap, sys, statistics, re, warningscsv_path = \"/mnt/data/file-UEosua5u86rkKhVexb1uLX-DEVRT-DACIA-SPRING.csv\"\n",
    "json_path = \"/mnt/data/file-MUUbFY99D7otoy1VxvfcUR-report.json\"\n",
    "\n",
    "# Load files\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "except Exception as e:\n",
    "    df = None\n",
    "    csv_err = str(e)\n",
    "\n",
    "try:\n",
    "    with open(json_path, \"r\") as f:\n",
    "        report = json.load(f)\n",
    "except Exception as e:\n",
    "    report = None\n",
    "    json_err = str(e)\n",
    "\n",
    "(type(df), len(df) if df is not None else None, type(report) if report is not None else None)# Inspect columns\n",
    "cols = list(df.columns) if df is not None else []\n",
    "colsimport pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_work = df.copy()\n",
    "\n",
    "# Ensure numeric types\n",
    "for col in [\"speedAvg\", \"soh\"]:\n",
    "    if col in df_work.columns:\n",
    "        df_work[col] = pd.to_numeric(df_work[col], errors=\"coerce\")\n",
    "\n",
    "# Fill missing speedAvg with mean\n",
    "if \"speedAvg\" in df_work.columns:\n",
    "    mean_speed = df_work[\"speedAvg\"].mean(skipna=True)\n",
    "    df_work[\"speedAvg\"] = df_work[\"speedAvg\"].fillna(mean_speed)\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0, 50, 80, 100, np.inf]\n",
    "labels = [\"0-50 km/h\", \"50-80 km/h\", \"80-100 km/h\", \"100+ km/h\"]\n",
    "\n",
    "# Assign bins\n",
    "# We consider only non-negative speeds; negative speeds will be clipped to 0 for binning\n",
    "if \"speedAvg\" in df_work.columns:\n",
    "    speeds = df_work[\"speedAvg\"].clip(lower=0)\n",
    "    df_work[\"speed_bin\"] = pd.cut(speeds, bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "# Compute average SOH per bin\n",
    "if \"soh\" in df_work.columns and \"speed_bin\" in df_work.columns:\n",
    "    soh_by_bin = df_work.groupby(\"speed_bin\")[\"soh\"].mean()\n",
    "    # Round to 2 decimals\n",
    "    soh_by_bin_rounded = soh_by_bin.round(2)\n",
    "    soh_by_bin, soh_by_bin_rounded, mean_speed, df_work[\"speed_bin\"].isna().sum()\n",
    "else:\n",
    "    soh_by_bin = None# Let's compute and return the series to view\n",
    "soh_by_bin_dict = None\n",
    "soh_by_bin_rounded_dict = None\n",
    "mean_speed_val = None\n",
    "n_na_speed_bin = None\n",
    "\n",
    "if \"soh\" in df_work.columns and \"speed_bin\" in df_work.columns:\n",
    "    soh_by_bin = df_work.groupby(\"speed_bin\")[\"soh\"].mean()\n",
    "    soh_by_bin_rounded = soh_by_bin.round(2)\n",
    "    soh_by_bin_dict = soh_by_bin.to_dict()\n",
    "    soh_by_bin_rounded_dict = soh_by_bin_rounded.to_dict()\n",
    "    mean_speed_val = float(df_work[\"speedAvg\"].mean())\n",
    "    n_na_speed_bin = int(df_work[\"speed_bin\"].isna().sum())\n",
    "\n",
    "soh_by_bin_dict, soh_by_bin_rounded_dict, mean_speed_val, n_na_speed_bin# Select bin with highest SOH based on rounded means\n",
    "if soh_by_bin_dict is not None:\n",
    "    soh_rounded = pd.Series(soh_by_bin_rounded_dict)\n",
    "    # Ensure ordering by our labels\n",
    "    soh_rounded = soh_rounded.reindex(labels)\n",
    "    optimal_bin = soh_rounded.idxmax() if not soh_rounded.dropna().empty else None\n",
    "else:\n",
    "    optimal_bin = None\n",
    "\n",
    "optimal_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88199c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "deleted_csv = client.files.delete(upload_csv.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_report = client.files.delete(upload_report.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
